{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1111f9fd-f715-4ed3-b1a2-c0985bf169af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, AutoTokenizer, AutoModelForMaskedLM, AutoConfig, AutoModel, AdamW, get_scheduler\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments, CamembertForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "from transformers.utils import ModelOutput\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9baa30be-3d11-4638-86dd-a06dc6c1547c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c9d8a7-4a1c-4aef-8393-ecc6c5aa04e8",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22e8ab88-1f68-4652-a73d-7e7a7a8b0750",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../tweets_labelled.csv\") # load labelled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78423091-cebe-46c8-a32e-25d71327760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['A supprimer'] != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f15f173d-81a7-4e15-b8b7-dc5d8af9e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove incomplete data\n",
    "df = df.dropna(subset=['Index', 'Sujet']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2b050a9-ed2f-4072-befb-314f3142c380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels: [0 0 0 ... 0 0 0]\n",
      "Decoded labels: ['Loin du VE' 'Loin du VE' 'Loin du VE' ... 'Loin du VE' 'Loin du VE'\n",
      " 'Loin du VE']\n"
     ]
    }
   ],
   "source": [
    "# transform categories to digits\n",
    "labels = df['Sujet']\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "print(\"Encoded labels:\", encoded_labels)\n",
    "\n",
    "decoded_labels = label_encoder.inverse_transform(encoded_labels)\n",
    "print(\"Decoded labels:\", decoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e9a7d-88cb-4e29-bc13-441bd204e819",
   "metadata": {},
   "source": [
    "## Hyperparameters and split sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bd279ee-ea1c-4e6d-b369-4020a82e24bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 10\n",
    "batch_size = 32\n",
    "max_seq_len = 128\n",
    "train_size = 0.6\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15dae9cc-9e95-4b08-84fa-e021236f3d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Texte'], encoded_labels, train_size=train_size, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee806d44-5f26-4154-95cb-1e4edf1c3064",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=random_seed) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65768df-7aaa-40d6-85bd-120ff7d17ad4",
   "metadata": {},
   "source": [
    "## Tokenization and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "692bcf94-d463-4b60-aa4f-3eabc25ff482",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"almanach/camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb4e22bd-89b7-48a8-8fca-ef6259793410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(texts):\n",
    "    return tokenizer(\n",
    "        texts, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=max_seq_len, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_function(X_train.tolist())\n",
    "val_encodings = tokenize_function(X_val.tolist())\n",
    "test_encodings = tokenize_function(X_test.tolist())\n",
    "\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_labels = torch.tensor(y_test)\n",
    "val_labels = torch.tensor(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15452930-257a-4c65-b545-17ce34eb9626",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create the train and test datasets\n",
    "train_dataset = TextDataset(train_encodings, train_labels)\n",
    "test_dataset = TextDataset(test_encodings, test_labels)\n",
    "val_dataset = TextDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e861aa7e-16d7-45c8-8ab4-9c00fd952cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle=True,\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf1ef57-599f-4766-be2f-3698ad70e3c9",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b39e9ab1-800c-4f48-8958-86795efb7f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and adapt bertweetfr-base to our 3-classes classification problem by changing the model head\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self,checkpoint,num_labels): \n",
    "        super(CustomModel,self).__init__() \n",
    "        self.num_labels = num_labels \n",
    "        \n",
    "        # Load Model at checkpoint\n",
    "        self.model = model = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "        self.dropout = nn.Dropout(0.1) \n",
    "        self.classifier = nn.Linear(768,num_labels) # load and initialize weights\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None,labels=None):\n",
    "        # Extract outputs from the body\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # custom layers\n",
    "        sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state\n",
    "        \n",
    "        logits = self.classifier(sequence_output[:,0,:].view(-1,768)) # compute loss\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        return ModelOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)\n",
    "\n",
    "\n",
    "    def save_model(self, name):\n",
    "        return self.model.save_model(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2645f205-e0d6-4a52-9b35-405bb6a814fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertModel were not initialized from the model checkpoint at Yanzhu/bertweetfr-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CustomModel(checkpoint=\"Yanzhu/bertweetfr-base\",num_labels=3).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "727085f7-0ec6-4d86-a5f1-359cb548b6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nina/miniconda3/envs/torch_cuda/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = nb_epoch\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dc3f568-88e8-4bf1-b097-fe55c56db622",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9441/3911851435.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"f1\")\n",
      "/home/nina/miniconda3/envs/torch_cuda/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "546a1a6a-e5d0-4276-a230-deb875771afe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544941ff73fb4ff19a09d4be178a2a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5de535e1a2447da94c82e6dc1932732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9441/1106817927.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "CamembertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 8.81 MiB is free. Process 9129 has 2.06 GiB memory in use. Including non-PyTorch memory, this process has 3.57 GiB memory in use. Of the allocated memory 3.42 GiB is allocated by PyTorch, and 45.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     16\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_cuda/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_cuda/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_cuda/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 8.81 MiB is free. Process 9129 has 2.06 GiB memory in use. Including non-PyTorch memory, this process has 3.57 GiB memory in use. Of the allocated memory 3.42 GiB is allocated by PyTorch, and 45.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "progress_bar_train = tqdm(range(num_training_steps))\n",
    "progress_bar_eval = tqdm(range(num_epochs * len(eval_dataloader)))\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch\", epoch+1)\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.append(loss.item())\n",
    "        progress_bar_train.update(1)\n",
    "    \n",
    "    print(\"Training\", np.mean(train_loss))\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "    \n",
    "        loss = outputs.loss\n",
    "        val_loss.append(loss.item())\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        progress_bar_eval.update(1)\n",
    "    \n",
    "    print(\"Validation\", np.mean(val_loss))\n",
    "    print(metric.compute(average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e18e37f-3068-4c41-a307-5d751aa79512",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb51fc46-e550-4d78-8a5a-5e638a261118",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "true = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    preds.append(predictions)\n",
    "    true.append(batch[\"labels\"])\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "res_metric = metric.compute(average='weighted')\n",
    "\n",
    "print('f1 :', round(res_metric['f1'], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e16b336-41a1-41fe-9b45-7b49e1c28fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "#model.model.save_pretrained(\"bertweetfr-retrained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c278a0a6-a6da-4522-a84b-ce02ebe718d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c0be1c-4c76-466f-aea6-706cbf8e0ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
